# Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations
This repository contains the survey materials (stimuli text, demographic questions, post-session questions) used to <em><strong>investigate human detection of varying degrees of LLM hallucination with and without warning tag</em></strong>.

The work is to support the [COLM 2024](https://colmweb.org/) paper ["Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations"](https://openreview.net/pdf?id=c30qeMg8dv) by Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee.

## Citation
If you find this repo helpful to your research, please cite the paper:
```
@inproceedings{nahar2024fakes,
title={Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding {LLM} Hallucinations},
author={Mahjabin Nahar and Haeseung Seo and Eun-Ju Lee and Aiping Xiong and Dongwon Lee},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=c30qeMg8dv}
}
```

# Download the dataset

You can check and download the [survey materials]() used in this study. 

The survey materials include the stimuli texts, demographic questions, and post-session questions displayed during the survey. The survey was conducted in Qualtrics. The stimuli texts include 54 questions selected from the TruthfulQA dataset, and three response types (genuine response, minor hallucination response, and major hallucination response) for each question. All responses have been generated using ChatGPT (GPT-3.5-Turbo) September 2023 version).
